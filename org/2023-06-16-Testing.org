#+OPTIONS: toc:nil
#+BEGIN_EXPORT html
---
layout: default
title: Testing 
subtitle: How to deliver code that works
---
#+END_EXPORT
#+TOC: headlines 2
* Testing
The single most important thing when delivering code is that it works. Testing is the process by which we prove that it works.

In the early stages of my career, I tested manually. I executed whatever process I was building in a test environment, and validated the outputs. I spun up web pages, and started clicking buttons. I monitored the goings-on between front-end and back-end with the Chrome Developer tools, and occasionally took a gander at my creations in Internet Explorer (the only browser which ran our universities ERP software. I'm not joking). Eventually, I would become satisfied, or my manager would become so dissastified with my pace of delivery, that I'd call it done. The business units I wrote software for would validate too.

I've since learned to think of testing as a pyramid. At the base of the pyramid are Unit Tests. These test small chunks of code in isolated slices.

These small chunks of code combine and amount to an application, which must fulfill certain requirements. We validate those requirements at the next level of the pyramid, with User Acceptance Tests. These tests aren't as isolated -- we let our code run end-to-end -- but we don't call any downstream services in the process. We stay in a vacuum, and test only the things which we can control.

Next are Contract Tests -- tests which validate that our code adheres to the contract defined in our documentation, and the contracts of the downstream services we consume. These ensure that our contact-points -- the areas which form the "boundaries" of our application -- are valid. This means testing the shape of the overall response of our application, and the requests which we send downwstream. Still, we don't call those downstream services, and we don't get called by any one upstream.

Finally at the tip of the pyramid are the integration tests. Tests which validate the entire system works from end to end. If we've done robust testing up until this point, these tests will be few in number, but will test critical paths.

With all of these bases covered, as engineers we should not stress over whether or not our code is ready (unless of course we wrote lazy usless tests to save time). At that point we do just need to ship our code.

* Unit Tests and Codebase Architecture
Unit Testing is an un-sung hero of software engineering. I've seen very few jr engineers get it right. Most codebases I've encountered are filled with lazily written do-nothing tests which dumbly satisfy code-coverage software. The best code bases are about 50:50, with a number of key tests which hold the ship together. I, as someone who learned about unit tests two years ago, will prove their value, and how to write them well. This is how I manage to sleep at night.

** Isolated Slices
The key thing to understand is that Unit Tests are intended to test isolated slices of code. They should test a single function, and the test should not ever leave that funcion. This is harder than it sounds. Writing good code means writing code that is intentionally written to be easy to test, and that takes some experience.

Take for example this code:

#+BEGIN_SRC python
CUSTOMERS_DB = [
    {"name": "Jennifer Love Hewitt", "age": 55},
    {"name": "Anthony Hopkins", "age": 34},
    {"name": "Martin Godzilla", "age": 67},
    {"name": "Tony Jabroney", "age": 21}
]

def create_customer_over_50_file():
    customers = CUSTOMERS_DB

    with open('../output/customers_over_50_report.txt', 'w') as writer:
        customers_over_50 = list(filter((lambda customer: customer['age'] > 50), customers))

        for customer in customers_over_50:
            writer.write(f'{customer["name"]}, {customer["age"]}\n')

create_customer_over_50_file()
#+END_SRC

This code accomplishes a fairly common set of tasks:
- Reads data from some data source
- Carries out some business logic (filtering and transforming the data in some necessary way)
- Writes it to an output destination

Regardless of whether or not you're connecting to a database, reading from an API, or streaming that data into a DLQ, the general principles remain. We access data, transform it, and output it.

The above code is nicely written. Generally, whomever wrote this function could feel pretty confident that it does what they want based on reading it. For many years, I wrote code just like this, and I would test it by manually reading the output data, and calling it done. "Customer over 50 report is ready for prod, boss".

But how do we /really/ test it? What if to ship we must achieve over 80% of the lines of code covered by a unit test?

No problem, I'd say. We could run the above function, and then run another function to validate. Job done, with the following code:

#+BEGIN_SRC python
import os

def validate_customer_over_50_file_exists():
    path = '../output/customers_over_50_report.txt'
    assert os.path.isfile(path), "Path should exist"

def validate_customer_over_50_file():
    with open('../output/customers_over_50_report.txt', 'r') as reader:
        line = reader.readline()
        while line != '':
            record = line.split(',')
            record = [v.strip() for v in record]
            assert int(record[1]) > 50, "Customer should be over 50"

            line=reader.readline()

validate_customer_over_50_file_exists()
validate_customer_over_50_file()
#+END_SRC

In some contexts, I'd see the above and would think... ship it. It's not awful. It has asserts which make sense at least. In my time at the university, this already exceeds any automated test I'd ever written (none).

But alas, at major software engineering organizations, this doesn't really fly.

Here's where it starts to break down for me:
- *Separation of Responsibility / Concerns*
  It's hard to detect in this short snippet, but recall there's 3 separate operations going on. There's the input, the business logic, and the output. They're all mixed together. Any of the 3 can evolve at any time. And there's times when from a software execution perspective, we're leaving the main execution, and we're waiting on I/O processes to complete. This is frowned upon... we're not here to unit test the python standard library.
- *Maintainability*
  Requirements change. One day, you're reading data from an in-memory source, but the next you're fetching it from a microservice. One day, it's customers over 50, the next, it's customers between the ages of 25 and 55 in California. One day, you're writing it to a flat file, but the next you're sending it to an API. In some cases, 100% of this code would need to be re-written.
- *Reusability*
  What if we need a second report. Much of the requirements are the same, but this time we're writing products to a CSV, and filtering by price. In the current design of this code, we would write a second, completely separate but eerily similiar function. And it would need all it's own unit tests.

You get the idea. It's not bad code, it's just not particularly /good/ code either. As far as products go, it's not well designed. It's like having a garlic-press in your kitchen. What it does, it does well. But it only does one thing, and you're gonna store it forever.

So what do we do? We abstract and refactor. This is where unit tests dictate our codebase architecture. When we develop code with unit tests in mind, the code is better organized, more reusable, and easier to maintain. My first boss at my new company once said, "Programming gets fun when you stop coding and start engineering". My eyes rolled all the way back into my head when he said that, but he was right. This is the part where engineers /design/ useful products.

** Refactoring for organization, maintainenace, and resuability
Recall that we are doing 3 primary things: fetching data, transforming it, and outputing it. These will form the basis for our refactor.

So, we'll write 3 classes which specialize in these areas. The first will be the CustomerRepository. It's sole purpose will be to surface customer data. The second will be the CustomerService, which is our business logic. This is where we'll implement what HR says they need -- for example, the filter for the customer age, and the string output format they require. Lastly, we'll write a FileWriter, whose sole job is to write data to files.

It is like follows:

#+BEGIN_SRC python
  # refactoring our code

  # stores and returns data
  class CustomerRepository:
      def __init__(self):
          pass

      def get_customers(self):
          return [
              {"name": "Jennifer Love Hewitt", "age": 55},
              {"name": "Anthony Hopkins", "age": 34},
              {"name": "Martin Godzilla", "age": 67},
              {"name": "Tony Jabroney", "age": 21}
          ]

  # data interactions
  class CustomerService:
      def __init__(self):
          pass

      def filter_customers(self, customers):
          return list(filter((lambda customer: customer['age'] > 50), customers))

      def get_customer_record_format(self, customer):
          return f'{customer["name"]}, { customer["age"]}\n'

  # writes to files
  class FileWriter():
      def __init__(self):
          pass

      def write_file(self, file_name, contents):
        #  An aside... using with "with open(file_name...) ... as " is called a "Context Manager"
        with open(file_name, 'w') as writer:
            for line in contents:
                writer.write(line + '\n')

  def main(customer_repository, customer_service, file_writer):
      customers = customer_repository.get_customers()
      customers_over_50 = customer_service.filter_customers(customers)
      formatted_customers_over_50 = [customer_service.get_customer_record_format(customer) for customer in customers_over_50]
      file_writer.write_file("../output/customers_over_50_report.txt", formatted_customers_over_50)

  main(CustomerRepository(), CustomerService(), FileWriter())
  validate_customer_over_50_file_exists()
  validate_customer_over_50_file()
#+END_SRC 

... and we can even re-use our old unit tests to check our work!

This is a lot more code, and it's got added complexity. We have classes now. We've effectively split the original code into its component parts. But each part is now reusable, easier to maintain, and easier to test. Let's take a look at those unit tests:

#+BEGIN_SRC python
def test_customer_repository(customer_repository):
    customers = customer_repository.get_customers()

    for customer in customers:
        assert customer["name"] != None and customer["age"] != None


def test_customer_service_filter(customer_service):
    customers = [
        {"name": "Jennifer Love Hewitt", "age": 55},
        {"name": "Anthony Hopkins", "age": 34},
    ]

    filtered_customers = customer_service.filter_customers(customers)

    assert len(filtered_customers) == 1

    for customer in filtered_customers:
        assert customer["age"] > 50


def test_customer_service_record_format(customer_service):
    customers = {"name": "Jennifer Love Hewitt", "age": 55}

    formatted_customer = customer_service.get_customer_record_format(customers)

    assert formatted_customer == "Jennifer Love Hewitt, 55", f"Should be like a CSV, but instead is {formatted_customer}"


test_customer_repository(CustomerRepository())
test_customer_service_filter(CustomerService())
test_customer_service_record_format(CustomerService())
#+END_SRC

The advantages to this code, even there's more complexity, are many. The functions are becoming "pure" functions. They are determinstic -- they do one thing and they do that one thing every time. Now, lots of teams can even re-use our CustomerRepository. We've put it in a central place, something data organizations strive for, for consistency. Other teams can use our FileWriter, so if we want to publish those products by price, much of the work is already done.


 Additionally, each component is now developed and tested separately. But notice that each test is simple. The components are isolated. The unit tests test only one function at a time. On an engineering team, we actually could've parallelized this work. Again imagine that fetching data is cumbersome, deciding the contract is a 3 week ordeal with product, and the business logic is the fun part (but product can't /really/ decide what they want. As tech lead, I can divy up this work. That's powerful for an organization. So powerful there's a [[https://en.wikipedia.org/wiki/Conway%27s_law][law about it]]. 

 But you may ask: where is the unit test for the FileWriter?

Here's the difficulty with the FileWriter. The execution of FileWriter leaves our code. It necessarily engages in some sort of File I/O that takes place externally from the exceution of our application. In unit tests, we have a rule: we don't leave the function we're testing.

To avoid this in unit tests, we use mocks. Mocking can be done in any language, but in python we commonly use ~unittest.mock~. Instead of opening a real file, we'll use ~mock_open~ and open a Mock file. Then we'll validate that our code interacts with the mock correctly -- but we won't actually write a file.

Isn't that bad? Not really -- at that point of writing to a file, it's executing code we didn't write. If an issue is found at that point, we'll open an Issue with the Python language folks. Here's what our mocked file_writer test looks like:

#+BEGIN_SRC python
from unittest.mock import patch, mock_open

def test_file_writer(file_writer):
    open_mock = mock_open()

    with patch("__main__.open", open_mock, create=True):
        file_writer.write_file("../output/test.txt", ["my-data"])

    open_mock.assert_called_once_with("../output/test.txt", "w")
    open_mock.return_value.write.assert_called_once_with("my-data\n")


test_file_writer(FileWriter())

#+END_SRC

In the above unit tests, we are using a mock to patch a function. In this case, we patch the ~open~ function with a new variable, ~open_mock~. This tells python, instead of calling ~open~ and creating a file, call our ~open_mock~ object instead. This is also how we build our asserts, since Python is monitoring the uses of ~open_mock~. We can therefore assert that it's called exactly once, and that we call it with ~my-data\n~, thereby enforcing our contract.

We can extend this further by testing "side-effects". We can test for the unforeseen circumstances. In the above code, if any exception occurs, the entire process fails. In many cases, that may be ok. And lots of times, for data pipelines, that's what we'd prefer to happen.

But we may want a more graceful exit for our program. And so, we mock with a side-effect. We tell python, when this function is invoked, raise an exception.

This is an example of side-effects in action:

#+BEGIN_SRC python
from unittest.mock import patch, mock_open

def test_file_writer_fails(file_writer):
    open_mock = mock_open()

    with patch("__main__.open", open_mock, create=True) as open_mock:
        open_mock.side_effect = IOError("Something bad happened!")
        file_writer.write_file("../output/test.txt", ["my-data"])

    open_mock.assert_called_once_with("../output/test.txt", "w")
    open_mock.return_value.write.assert_called_once_with("my-data\n")

test_file_writer_fails(FileWriter())

#+END_SRC

The above unit code, run against the current version of FileWriter, will fail with the following:

#+BEGIN_SRC
Traceback (most recent call last):
  File "/Users/maxfrey/Development/blog/msfrey.github.io/notes/python/untestable.py", line 140, in <module>
    test_file_writer_fails(FileWriter())
  File "/Users/maxfrey/Development/blog/msfrey.github.io/notes/python/untestable.py", line 135, in test_file_writer_fails
    file_writer.write_file("../output/test.txt", ["my-data"])
  File "/Users/maxfrey/Development/blog/msfrey.github.io/notes/python/untestable.py", line 69, in write_file
    with open(file_name, 'w') as writer:
         ^^^^^^^^^^^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py", line 1118, in __call__
    return self._mock_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py", line 1122, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py", line 1177, in _execute_mock_call
    raise effect
OSError: Something bad happened!
#+END_SRC

This a wonderful thing. We've told python, simulate an exception. Simulate the unforeseen issues that can arise. Maybe the disk runs out of memory. Maybe this is all happening in the cloud, and our disk phsyically fails. These things can and do happen.

So, we'll write some code to improve our FileWriter. For example, say we want to output a log message, and end execution:

#+begin_src python
  # writes to files, and handles exceptions more gracefully.
  class FileWriter():
      def __init__(self):
          pass

      def write_file(self, file_name, contents):

          try:
              with open(file_name, 'w') as writer:
                  for line in contents:
                      writer.write(line + '\n')
          except IOError as ex:
              print(f"An error has occured while writing {file_name}")
              return

  # unit test with exception raised!
  from unittest.mock import patch, mock_open

  def test_file_writer_fails(file_writer):
      open_mock = mock_open()

      with patch("__main__.open", open_mock, create=True) as open_mock:
          open_mock.side_effect = IOError("Something bad happened!")
          file_writer.write_file("../output/test.txt", ["my-data"])

      open_mock.assert_called_once_with("../output/test.txt", "w")
      open_mock.return_value.write.assert_not_called()

  test_file_writer_fails(FileWriter())

#+end_src


Now we're in some serious business -- dare I say, we've encountered some /best practices/. /*Robust*/ practices. Do you need to do this in your personal project? Probably not. But in productionalized code for a big company? Ace.

** Additional powers of abstraction
In each of these cases, CustomerRepository, CustomerService, and FileWriter, we're abstracting away the details. The main function is dependent of these external classes, but it's agnostic toward the implementation details. For example, the main() function doesn't care what happens inside the CustomerRepository, main just cares that it gets data back in the format it requires. It doesn't care what format the CustomerService prints customers in, just that it returns a list of strings.

This leads to some powerful capabilities, namely in the customization of code. For example, what if the CustomerRepository was no longer stored in memory, but instead became a database? Well, there would be no changes to main. As long as the database driver implements a CustomerRepository, containing the function ~get_customers~, it can be anything it wants to be. It can read from a file, it can fetch from an API... main doesn't care. Main will work all the same.

In much the same way any CD can be inserted into a disc drive, any CustomerRepository can be inserted into main.

This is subsequently where mocks can and will come back into play. Let's say CustomerRepository does turn into a full-fledged cloud-provided database. In unit tests, we no longer want to make the actual database call. So, we'll pass in a Mock for those aspects of the call to ~get_customers~, much the same way we did with ~open~. 

Unit Testing our ~~main()~~ function is also necessary. In this case, it's just going to be a series of mocks however. Remember: we test isolated slices. The unit test for main should not leave main. You might say, "but that's not testing anything at all!" and you'd be right -- it's really not in this case. In it's current state, main cares only about contracts. That is to say, it needs data in a very specific format, and it needs to stay in that format even as say, files turn into databases. More on contract testing later.


* Component Testing
So far, we've kept our code in a vacuum intentionally. At this point, we're confident that the code itself is working, and we've even enhanced a few components. We've parallelized the work among engineering teams, they've implemented their components, and now we're putting the pieces together. Component testing will validate that the components we've built work when combined.

In Component Testing, our boundaries of execution expand a bit. Contract testing entails testing the code from end-to-end, but not to the extent of leaving the machine. What I mean by that is, we'll execute main, without mocking the /internal/ dependencies (CustomerRepository, CustomerService, FileWriter). We won't mock the file writer either--we should get some output in a file. However, I would still recommend that we mock any dependencies that leave our machine. For example, if we were calling a database or an API, that should stay mocked. If the file needs to be transered to S3 for example... don't do that.  Our boundaries are expanding, but only somewhat.

There are several reasons for this. Component testing is about satisfying requirements. This is where we demonstrate to product, "look, this application does everything you asked." To accomplish this, we need to test edge cases. We need data that looks normal and we need data that looks funny. We need odd characters that might trip us up (god forbid someone has a comma in their name...). We need some control over the scenarios we test against, so that product trusts the application is code-complete.

The mocks for our downstreams also become more complex here. We may for example mock network timeouts at this stage, and validate the response of the entire application in the result of that timeout. We may mock a 404 error here, and again, validate the behavior of the entire application is as expected.

To make this testing more repeatable and reliable, we don't hit the database or any APIs. There's also a cost / etiquette implication here. Imagine that database or API is owned by a separate team. They don't want your QA traffic hammering their DB everytime your junior engineer tries to push a new feature into the development environment. You're spending too much money testing something which isn't fully tested yet.



* Contract Testing
Contract testing is at a similiar level to Component Testing / User Acceptance Testing. Again, we're right up against the boundaries of our application code: the final response object of an API for example, or the HTTP Request we're sending down stream (ie, we're satisfying the downstream partner's contract too).

These tests ensure that those plug-in points are working. There's a number of platforms out there to enable this, but perhaps simplest is to ensure that your component tests mock the downstream dependencies, but have specific assertions that fully capture the contact-points are working.

New ways of testing contracts are becoming available, as major companies move to micro-serice architectures. 

* Integration Tests
These are the real deal tests. Now, we can start hitting our downstream dependencies, finally. It's time to let them know to expect some traffic.

These tests are fewer in number. We mainly are concerned that we actually can communicate with the outside world. This is NOT the time to validate edge cases. We've already done that. 
